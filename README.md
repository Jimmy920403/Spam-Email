## Project: Spam Email Classification (Phase 1 Baseline)

This repository demonstrates a traditional ML baseline for spam (SMS/Email) classification. It includes OpenSpec change proposals, data utilities, training, model export, visualization, a Streamlit app, and a minimal CI workflow.

> Repo: Spam-Email (HW3)

### Highlights
- Purpose: Reproducible SVM baseline with a text-vectorization pipeline.
- Executables: dataset download, preprocessing, training, prediction, and visualization.
- UI: `apps/streamlit_app.py` provides live inference and charts.
- CI: `.github/workflows/ml-baseline.yml` runs a fast smoke test with an F1 metric gate.
- Specs: OpenSpec change proposals live under `openspec/changes/`.

## Layout (key folders/files)
- `scripts/` — CLI scripts (train, preprocess, download, predict, visualize).
- `apps/` — Streamlit app module (`apps/streamlit_app.py`).
- `models/` — Trained model artifacts (joblib).
- `artifacts/` — Metrics and intermediate outputs (JSON, images, etc.).
- `reports/visualizations/` — Plots generated by the visualization script (ROC, PR, CM, token frequency, etc.).
- `data/` — Raw and sample data (`data/sample.csv`, `data/sms_spam_no_header.csv`).
- `openspec/` — Project specs and change proposals (`project.md`, `changes/`).
- `tests/` — Pytest tests.
- `requirements.txt` — Python dependencies.

## Quickstart (Windows PowerShell)
1) Create and activate a virtual environment, then install deps:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
pip install -r requirements.txt
```

2) Run tests:

```powershell
pytest -q
```

3) Smoke-train on the sample data (fast):

```powershell
python -m scripts.train_baseline --data data/sample.csv --model-out models/baseline-svm.joblib --metrics-out artifacts/metrics.json --cm-out artifacts/confusion_matrix.png
```

4) Train on the full Packt dataset (local/off-CI):

```powershell
python -m scripts.train_baseline --data data/sms_spam_no_header.csv --model-out models/pipeline.joblib --metrics-out artifacts/metrics_full.json --cm-out artifacts/confusion_matrix_full.png
```

5) Predict via CLI (single text example):

```powershell
python scripts/predict_spam.py --model models/pipeline.joblib --text "Free entry in 2 a wkly comp to win cash"
```

6) Launch the Streamlit app:

```powershell
streamlit run apps/streamlit_app.py
```

## Artifacts
- Models: `.joblib` files under `models/` (recommend `pipeline.joblib`, which packs vectorizer + classifier).
- Metrics: `artifacts/metrics*.json`.
- Visualizations: PNGs and threshold-sweep CSV in `reports/visualizations/`.

## OpenSpec and proposals
Design and change management lives in `openspec/`. Proposals are in `openspec/changes/`. See `openspec/project.md` and `openspec/AGENTS.md` for how to propose and review changes.

## Dev & contribution tips
1) Before coding a new feature, start an OpenSpec proposal draft (`openspec/changes/<your-change>/proposal.md`).
2) When implementing: add tests under `tests/` and keep a tiny sample dataset so CI stays fast.
3) Prefer exporting an sklearn `Pipeline` (vectorizer + classifier) for stable serialization and inference.
4) CI principle: keep smoke tests quick; full training can run locally or on a dedicated runner.

## FAQ / notes
- Prefer running scripts as modules to avoid import-path issues: `python -m scripts.train_baseline`. If running files directly, ensure the project root is on `PYTHONPATH`.
- If you hit unpickle errors, verify the model is a `Pipeline` and the source layout hasn’t moved. You can also load via `joblib` and reconstruct a `Pipeline` if needed.

## Next steps
- Optionally surface images from `reports/visualizations` inside `apps/streamlit_app.py` (display + download links).
- Add a more thorough preprocessing pipeline with step-by-step saved outputs for traceability.

## Contact / support
- OpenSpec discussion and change proposals: create a new item under `openspec/changes/`.
- If you want me to extend the app (charts integration, docs, CI badges), just say the word.

---
Last updated: 2025-11-03
